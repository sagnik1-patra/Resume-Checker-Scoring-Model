{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4ccb529-182d-4ef3-9d37-6aee2d9e8edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " All files created in: C:\\Users\\sagni\\Downloads\\Resume Selector\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#  Create project folder\n",
    "project_dir = r\"C:\\Users\\sagni\\Downloads\\Resume Selector\"\n",
    "os.makedirs(project_dir, exist_ok=True)\n",
    "\n",
    "#  Training script\n",
    "train_code = \"\"\"\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import spacy\n",
    "\n",
    "#  Paths\n",
    "data_path = r\"C:\\\\Users\\\\sagni\\\\Downloads\\\\Resume Selector\\\\UpdatedResumeDataSet.csv\"\n",
    "model_save_path = r\"C:\\\\Users\\\\sagni\\\\Downloads\\\\Resume Selector\\\\resume_model\"\n",
    "\n",
    "#  Load dataset\n",
    "df = pd.read_csv(data_path, encoding='utf-8')\n",
    "df = df[['Category', 'Resume']]\n",
    "df.dropna(inplace=True)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "#  Encode labels\n",
    "labels = df['Category'].unique()\n",
    "label2id = {label: idx for idx, label in enumerate(labels)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "df['label'] = df['Category'].map(label2id)\n",
    "\n",
    "#  Text preprocessing\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "def preprocess(text):\n",
    "    doc = nlp(str(text).lower())\n",
    "    return ' '.join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct])\n",
    "\n",
    "df['clean_resume'] = df['Resume'].apply(preprocess)\n",
    "\n",
    "#  Dataset class\n",
    "class ResumeDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=256):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encodings = self.tokenizer(self.texts.iloc[idx], truncation=True, padding='max_length', max_length=self.max_len, return_tensors='pt')\n",
    "        return {key: val.squeeze(0) for key, val in encodings.items()}, torch.tensor(self.labels.iloc[idx])\n",
    "\n",
    "#  Tokenizer & Model\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(labels))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "#  Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['clean_resume'], df['label'], test_size=0.2, random_state=42)\n",
    "train_dataset = ResumeDataset(X_train, y_train, tokenizer)\n",
    "test_dataset = ResumeDataset(X_test, y_test, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4)\n",
    "\n",
    "#  Training\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "epochs = 2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        inputs, labels_batch = batch\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        labels_batch = labels_batch.to(device)\n",
    "\n",
    "        outputs = model(**inputs, labels=labels_batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "#  Evaluation\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs, labels_batch = batch\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        logits = model(**inputs).logits\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        y_pred.extend(preds)\n",
    "        y_true.extend(labels_batch.numpy())\n",
    "\n",
    "print(\"\\\\n Classification Report:\\\\n\", classification_report(y_true, y_pred, target_names=labels))\n",
    "\n",
    "#  Save model & tokenizer\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "print(f\" Model and tokenizer saved to '{model_save_path}'\")\n",
    "\"\"\"\n",
    "\n",
    "#  Prediction script\n",
    "predict_code = \"\"\"\n",
    "import torch\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "import fitz  # PyMuPDF\n",
    "import spacy\n",
    "\n",
    "#  Load model & tokenizer\n",
    "model_dir = r\"C:\\\\Users\\\\sagni\\\\Downloads\\\\Resume Selector\\\\resume_model\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_dir)\n",
    "model = BertForSequenceClassification.from_pretrained(model_dir)\n",
    "model.eval()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "#  Role-specific keywords\n",
    "role_keywords = {\n",
    "    'Data Science': ['python', 'machine learning', 'pandas', 'tensorflow'],\n",
    "    'Python Developer': ['python', 'flask', 'django', 'api'],\n",
    "    'Java Developer': ['java', 'spring', 'hibernate'],\n",
    "}\n",
    "\n",
    "#  Preprocess function\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "def preprocess(text):\n",
    "    doc = nlp(str(text).lower())\n",
    "    return ' '.join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct])\n",
    "\n",
    "#  Extract text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text.strip()\n",
    "\n",
    "#  Predict and analyze\n",
    "def predict_resume_from_pdf(pdf_path, threshold=0.85):\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    if not text:\n",
    "        print(\" No text found in the PDF!\")\n",
    "        return\n",
    "    \n",
    "    print(\" Extracted Resume Text (first 500 chars):\\\\n\", text[:500], \"...\\\\n\")\n",
    "\n",
    "    clean_text = preprocess(text)\n",
    "    encoding = tokenizer(clean_text, truncation=True, padding='max_length', max_length=256, return_tensors='pt')\n",
    "    encoding = {k: v.to(device) for k, v in encoding.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(**encoding).logits\n",
    "        probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
    "        pred_idx = torch.argmax(torch.tensor(probs)).item()\n",
    "        confidence = probs[pred_idx]\n",
    "        predicted_category = list(role_keywords.keys())[pred_idx]\n",
    "\n",
    "    print(f\" Predicted Category: {predicted_category}\")\n",
    "    print(f\" Confidence Score: {confidence * 100:.2f}%\")\n",
    "\n",
    "    #  Keyword Analysis\n",
    "    resume_words = set(clean_text.split())\n",
    "    keywords = set(role_keywords.get(predicted_category, []))\n",
    "    present_keywords = resume_words & keywords\n",
    "    missing_keywords = keywords - resume_words\n",
    "\n",
    "    print(f\" Found keywords: {', '.join(present_keywords) if present_keywords else 'None'}\")\n",
    "    if missing_keywords:\n",
    "        print(f\" Missing important keywords: {', '.join(missing_keywords)}\")\n",
    "    else:\n",
    "        print(\" All key skills present!\")\n",
    "\n",
    "#  Example Usage\n",
    "pdf_resume_path = r\"C:\\\\Users\\\\sagni\\\\Downloads\\\\Resume NextWave\\\\Resume.pdf\"\n",
    "predict_resume_from_pdf(pdf_resume_path)\n",
    "\"\"\"\n",
    "\n",
    "#  Requirements\n",
    "requirements = \"\"\"\n",
    "torch\n",
    "transformers\n",
    "spacy\n",
    "pandas\n",
    "scikit-learn\n",
    "PyMuPDF\n",
    "\"\"\"\n",
    "\n",
    "# Save files\n",
    "with open(os.path.join(project_dir, \"train_resume_model.py\"), \"w\") as f:\n",
    "    f.write(train_code.strip())\n",
    "\n",
    "with open(os.path.join(project_dir, \"predict_resume.py\"), \"w\") as f:\n",
    "    f.write(predict_code.strip())\n",
    "\n",
    "with open(os.path.join(project_dir, \"requirements.txt\"), \"w\") as f:\n",
    "    f.write(requirements.strip())\n",
    "\n",
    "print(\" All files created in:\", project_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e1c2c09-3a8d-4049-b70c-ab5182430af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Web app created in: C:\\Users\\sagni\\Downloads\\Resume Selector\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "project_dir = r\"C:\\Users\\sagni\\Downloads\\Resume Selector\"\n",
    "templates_dir = os.path.join(project_dir, \"templates\")\n",
    "static_dir = os.path.join(project_dir, \"static\")\n",
    "os.makedirs(templates_dir, exist_ok=True)\n",
    "os.makedirs(static_dir, exist_ok=True)\n",
    "\n",
    "#  Flask app (app.py)\n",
    "app_code = \"\"\"\n",
    "from flask import Flask, render_template, request, jsonify\n",
    "import torch\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_dir = os.path.join(os.path.dirname(__file__), \"resume_model\")\n",
    "model = BertForSequenceClassification.from_pretrained(model_dir)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_dir)\n",
    "label2id = {'Data Science': 0, 'HR': 1, 'Advocate': 2, 'Arts': 3, 'Web Designing': 4,\n",
    "            'Mechanical Engineer': 5, 'Sales': 6, 'Health and fitness': 7, 'Civil Engineer': 8,\n",
    "            'Java Developer': 9, 'Business Analyst': 10, 'SAP Developer': 11, 'Automation Testing': 12,\n",
    "            'Electrical Engineering': 13, 'Operations Manager': 14, 'Python Developer': 15,\n",
    "            'DevOps Engineer': 16, 'Network Security Engineer': 17, 'PMO': 18, 'Database': 19,\n",
    "            'Hadoop': 20, 'ETL Developer': 21, 'DotNet Developer': 22, 'Blockchain': 23, 'Testing': 24}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text.strip()\n",
    "\n",
    "def predict_resume(text):\n",
    "    encoding = tokenizer(text, truncation=True, padding='max_length', max_length=256, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        logits = model(**encoding).logits\n",
    "        probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
    "        pred_idx = probs.argmax()\n",
    "        confidence = probs[pred_idx]\n",
    "    return id2label[pred_idx], confidence\n",
    "\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    return render_template(\"index.html\")\n",
    "\n",
    "@app.route(\"/predict\", methods=[\"POST\"])\n",
    "def predict():\n",
    "    file = request.files[\"resume\"]\n",
    "    if not file:\n",
    "        return jsonify({\"error\": \"No file uploaded\"}), 400\n",
    "\n",
    "    file_path = os.path.join(\"uploads\", file.filename)\n",
    "    os.makedirs(\"uploads\", exist_ok=True)\n",
    "    file.save(file_path)\n",
    "\n",
    "    text = extract_text_from_pdf(file_path)\n",
    "    category, confidence = predict_resume(text)\n",
    "\n",
    "    os.remove(file_path)  # Clean up uploaded file\n",
    "    return jsonify({\n",
    "        \"category\": category,\n",
    "        \"confidence\": f\"{confidence*100:.2f}%\"\n",
    "    })\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)\n",
    "\"\"\"\n",
    "\n",
    "#  HTML frontend (index.html)\n",
    "html_code = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <title>Resume Classifier</title>\n",
    "    <style>\n",
    "        body { font-family: Arial, sans-serif; text-align: center; margin-top: 50px; }\n",
    "        input[type=file], button { padding: 10px; margin: 10px; }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Resume Role Predictor</h1>\n",
    "    <form id=\"uploadForm\" enctype=\"multipart/form-data\">\n",
    "        <input type=\"file\" name=\"resume\" accept=\".pdf\" required><br>\n",
    "        <button type=\"submit\">Predict Role</button>\n",
    "    </form>\n",
    "    <div id=\"result\"></div>\n",
    "\n",
    "    <script>\n",
    "        document.getElementById(\"uploadForm\").onsubmit = async function(event) {\n",
    "            event.preventDefault();\n",
    "            const formData = new FormData(this);\n",
    "            const response = await fetch(\"/predict\", { method: \"POST\", body: formData });\n",
    "            const result = await response.json();\n",
    "            if (result.error) {\n",
    "                alert(result.error);\n",
    "            } else {\n",
    "                document.getElementById(\"result\").innerHTML =\n",
    "                    `<h2> Predicted Role: ${result.category}</h2>\n",
    "                     <h3> Confidence: ${result.confidence}</h3>`;\n",
    "            }\n",
    "        };\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Write files with utf-8 encoding\n",
    "with open(os.path.join(project_dir, \"app.py\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(app_code.strip())\n",
    "\n",
    "with open(os.path.join(templates_dir, \"index.html\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(html_code.strip())\n",
    "\n",
    "print(\" Web app created in:\", project_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c69fdd-2a84-4ef7-9665-2373d8fd8a87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (moviepy)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
